come up with a plan to deploy this project to Hetzner, it should include:
- The services required to run and scale the game economically.
- A cost-first autoscaling strategy to support 100,000 concurrent players (CCU).
- The gap between current code and target architecture, including concrete work items.
- approximate monthly cost of the cloud bills

Interpretation note:
- The request "10,0000 players" is treated as 100,000 concurrent players.

Planning defaults selected:
- Cloud model: provider-neutral but the cheapeast easiest provider to work with.
- Performance/cost posture: cost-first baseline.
- Rollout: US-first, then global.
- Room access: auto-join by default, manual room switching still supported.


============================================================
2) Current Stack (As Implemented in Repo)
============================================================

Frontend (client)
- Vite + React + TypeScript + Three.js.
- Deployed as static assets via Cloudflare Worker.
- Worker only serves frontend assets; no multiplayer state in Worker.
- Backend URL is configurable with VITE_BACKEND_URL.
- WebSocket URL is derived from backend URL (ws/wss).

Backend (authoritative game server)
- Rust, Tokio, Axum.
- WebSocket endpoint: GET /api/room/:room.
- HTTP endpoints:
  - GET /api/health
  - GET /api/leaderboard
  - POST /api/leaderboard
  - POST /api/debug/kill (only when ENABLE_DEBUG_COMMANDS=1)
- Room state is in-process memory, keyed by room name in a DashMap.
- Room game loop is authoritative and ticks at 20 Hz (TICK_MS = 50).
- Snapshot fanout is per-session and view-scoped (server culls snake windows/pellets by view).

Realtime protocol
- Custom binary WebSocket protocol, version = 11.
- Client still accepts/handles some legacy JSON behavior, but server sends binary.
- Message stream includes init/state/meta payloads and per-player scoreFraction/oxygen/girth/tailExtension fields.

Persistence
- Leaderboard storage is SQLite via sqlx.
- Migration-managed schema (scores table + score index).
- Current backend defaults to sqlite://data/leaderboard.db unless DATABASE_URL is set.

Operational notes from current code
- Backend default listen port is 8787 (PORT env override).
- CORS is currently permissive (Any origin, GET/POST allowed).
- No explicit room capacity limit in current room logic.
- No global room directory service.
- No built-in autoscaler integration.
- No dedicated metrics/alerts pipeline in code.


============================================================
3) Current Scaling Limits / Risks
============================================================

1. Room placement is local-only:
- Rooms are created on first join in each backend process.
- There is no cross-instance registry to route players to the least-loaded room.

2. No hard room cap:
- Current server does not reject joins at capacity thresholds.
- A popular room can become oversized and degrade tick latency.

3. Single-node SQLite is fine for low write volume, but:
- Not ideal for HA/failover and multi-instance write patterns at production scale.

4. No autoscaling control plane:
- No service currently computes "room full => place in another room" globally.

5. Missing production-grade observability:
- Limited ability to scale by objective SLOs (tick overrun, WS join latency, reconnect rates).


============================================================
4) Target Production Architecture (Provider-Neutral)
============================================================

Core services required
1. Edge Static Delivery
- CDN + edge runtime for frontend static assets.
- Current fit: existing Cloudflare Worker static asset serving.

2. API/WS Ingress Gateway
- Global or regional L7 ingress that supports WebSockets.
- TLS termination, routing, rate limiting, optional stickiness.

3. Matchmaker API (new)
- Auto-assigns player to an available room shard.
- Creates or selects room based on capacity, health, and region.
- Returns roomId + wsUrl + joinToken.

4. Room Directory / Presence Store (new)
- Fast ephemeral store for room occupancy, state, and heartbeat data.
- Redis-compatible managed service recommended.

5. Game Room Server Fleet
- Horizontally scaled Rust backend instances.
- Each instance runs multiple rooms in-process.
- Room lifecycle states: WARM, ACTIVE, DRAINING, TERMINATED.

6. Leaderboard API + Persistent DB
- Keep leaderboard API route in backend or split to a small API service.
- Move persistence to managed Postgres-compatible DB for HA and operations.

7. Observability Platform
- Metrics, structured logs, traces, dashboards, alerts.
- Required for autoscaling and incident response.

8. CI/CD + IaC
- Infrastructure as code (Terraform/Pulumi) and controlled deployment pipeline.

9. Secrets and Config Management
- Managed secret store for DB credentials, JWT/HMAC keys, service tokens.

Optional later services (not day-one mandatory)
- Regional traffic director for global latency optimization.
- Queue/event bus for async analytics pipelines.
- DDoS/bot hardening layers beyond basic WAF/rate controls.


============================================================
5) Room Sharding and Auto-Join Model
============================================================

Player flow (default)
1. Client calls POST /api/matchmake.
2. Matchmaker selects best room shard in preferred region.
3. Matchmaker returns wsUrl + roomId + short-lived joinToken.
4. Client connects to GET /api/room/:room and includes joinToken in join frame.

Manual room switching
- Keep manual room selection for party play/testing.
- If a manual room is full or DRAINING, matchmaker returns a redirect recommendation.

Capacity defaults (cost-first baseline)
- Soft cap per room: 120 human players.
- Hard cap per room: 140 human players.
- At hard cap, room rejects new joins.
- Keep 15% global spare room capacity for bursts/reconnect storms.

Sizing math
- 100,000 CCU / 120 = 833.33 => 834 active rooms baseline.
- With 15% spare capacity: target around 959 room slots worth of active capacity.
- Exact rooms-per-instance depends on load tests and instance profile.

Room lifecycle policy
- WARM: empty/low-load room ready for assignment.
- ACTIVE: receiving normal joins.
- DRAINING: no new joins, existing sessions continue or migrate.
- TERMINATED: removed after idle/drain completion.


============================================================
6) Required API and Protocol Changes
============================================================

A) New public endpoint: POST /api/matchmake
Request (example):
{
  "playerId": "optional-existing-player-id",
  "playerName": "optional-display-name",
  "preferredRegion": "us-east",
  "roomOverride": "optional-manual-room"
}

Response (example):
{
  "roomId": "main-0421",
  "wsUrl": "wss://game-us.example.com/api/room/main-0421",
  "joinToken": "short-lived-signed-token",
  "expiresAt": 1760000000000,
  "region": "us-east"
}

B) Internal heartbeat endpoint (service-to-service)
- POST /internal/room-heartbeat
- Payload includes:
  - roomId
  - region
  - humanPlayers
  - botPlayers
  - state (WARM/ACTIVE/DRAINING)
  - tickLagMs p95
  - joinable boolean
  - instanceId

C) Protocol update (v12)
- Add optional joinToken to binary join message.
- Server validates token before accepting/joining session.
- Maintain temporary compatibility window for v11 + v12 during rollout.


============================================================
7) Autoscaling Strategy
============================================================

Fleet autoscaling signals (game server deployment)
- CPU utilization.
- Memory utilization.
- Active WebSocket connections.
- Network egress.
- Tick-loop health:
  - tick overrun rate
  - p95/p99 tick duration
- Room pressure:
  - count of rooms above soft cap
  - available WARM room count

Scale-out policy
- Trigger on sustained pressure (for example 1-3 minutes) across at least two key signals.
- Always keep minimum warm capacity buffer (15% spare).

Scale-in policy
- Only scale in when:
  - sustained low usage,
  - enough warm spare capacity remains,
  - no room currently marked DRAINING due to node retirement.

Placement policy
- US-first regional pools.
- Prefer placing players in-region with lowest join latency and available room capacity.


============================================================
8) Cost Controls (Important)
============================================================

1. Bin-pack rooms per instance
- Avoid one-room-per-instance.
- Run multiple rooms per process/instance up to validated thresholds.

2. Keep control plane lightweight
- Matchmaker + room directory should be simple, stateless where possible, and horizontally scalable.

3. Separate hot and cold data concerns
- Gameplay state stays in-memory (cheap, fast, ephemeral).
- Persist only what must survive (leaderboard, audit/analytics aggregates).

4. Conservative regional footprint
- Start US-first.
- Add regions only when telemetry shows sustained demand.

5. Guardrails
- Enforce room caps.
- Disable or reduce bot population automatically in high-load rooms.
- Reap long-idle rooms aggressively after grace window.


============================================================
9) Reliability, Security, and Operations
============================================================

Reliability requirements
- Graceful shutdown/drain support:
  - mark instance DRAINING,
  - stop new joins,
  - allow active sessions to complete or migrate.
- Health/readiness probes must include room joinability and event-loop pressure.

Security requirements
- TLS end-to-end.
- API rate limits (matchmake, leaderboard submit, join attempts).
- Join token TTL (15-30s) and single-use or nonce protection.
- Basic abuse controls for room spam and reconnect floods.

Observability minimums
- Dashboards:
  - CCU by region
  - room occupancy distribution
  - join latency p50/p95/p99
  - tick duration/overrun
  - reconnect/error rates
  - egress by region/service
- Alerts:
  - sustained tick overruns
  - join failure spike
  - low warm capacity buffer
  - DB/Redis health degradation


============================================================
10) Rollout Plan
============================================================

Phase 0: Foundation
- Add matchmaker service and room directory store.
- Add room cap enforcement + room lifecycle states.
- Add metrics and dashboards.

Phase 1: US Pilot
- Route small percentage of users through auto-matchmake flow.
- Keep manual room override and v11 compatibility.

Phase 2: US Full
- Default all traffic to auto-join.
- Enable dynamic room creation and drain-aware scale-in.
- Keep manual room switching as feature, subject to capacity rules.

Phase 3: Global Expansion
- Add additional regions with latency-aware placement.
- Tune per-region warm capacity and cost budgets.


============================================================
11) Acceptance Tests and Load Validation
============================================================

Functional tests
1. Matchmaker returns valid room + token for new session.
2. Hard room cap blocks additional joins.
3. New room is created when no room is under soft cap.
4. Manual room override works when joinable.
5. DRAINING room rejects new joins.

Compatibility tests
6. Protocol v11 clients still connect during migration window.
7. Protocol v12 join token validation works and blocks invalid/expired tokens.

Performance and resilience tests
8. 100k CCU synthetic load across room shards with stable join success rates.
9. Tick-loop SLO stays within target over long soak periods.
10. Node drain test causes no cascading disconnect failures.
11. Regional outage simulation reroutes new joins and preserves service continuity.

Cost tests
12. Scale-in preserves spare capacity while reducing off-peak compute costs.


============================================================
12) Implementation Backlog (Concrete Work Remaining)
============================================================

Priority P0 (must-have before real high-scale launch)
- Add room capacity constants and join enforcement to backend room join path.
- Build POST /api/matchmake endpoint and a room directory integration.
- Add short-lived join tokens and protocol v12 support.
- Introduce managed Redis-compatible room state store.
- Introduce managed Postgres-compatible leaderboard DB (replace SQLite in prod).
- Add metrics instrumentation around tick, fanout, and room occupancy.

Priority P1
- Add room drain lifecycle and zero-downtime scale-in behavior.
- Add autoscaler policies based on room pressure + tick SLO.
- Add load-test harness that simulates realistic WS players.

Priority P2
- Multi-region expansion and latency-aware routing.
- Advanced abuse mitigation and adaptive per-room bot suppression.


============================================================
13) Quick Service Checklist for Cloud Engineer
============================================================

Required at launch (100k target path)
- Edge static hosting/CDN for frontend.
- WebSocket-capable ingress/load balancer.
- Matchmaker API service.
- Room directory/presence data store (Redis-compatible).
- Autoscaled game server fleet for Rust backend.
- Managed relational DB for leaderboard.
- Centralized logs/metrics/traces + alerting.
- CI/CD and infrastructure-as-code pipeline.
- Secret management.

Can be deferred
- Event streaming/queue for analytics.
- Global multi-region deployment beyond US.
- Deep anti-abuse stack beyond baseline WAF/rate limits.


End of handoff.
